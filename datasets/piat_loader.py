import torch
import functools
import piat  # å‡è®¾ piat åº“å·²å®‰è£…
import json
import os
from PIL import Image
import sys
import random
import numpy as np
from torchvision import transforms

import piat, functools

from datasets.piat_core import Dataloader
# å¯¼å…¥ bucket dataloader ç”¨äºæœ¬åœ°å›¾åƒè®­ç»ƒ
from datasets.bucket_dataloader import create_dataloader as create_bucket_dataloader


# def degrade_image(pil_image, mode='2x'):
#     if mode == '2x':
#         # å…ˆ4å€ä¸‹é‡‡æ ·å†ä¸Šé‡‡æ ·
#         w, h = pil_image.size
#         down_w, down_h = max(1, w // 2), max(1, h // 2)
#         img = pil_image.resize((down_w, down_h), resample=Image.Resampling.BICUBIC)
#         return img
  

def attach_images(objSettings, objScratch, objOutput):
    objSettings = objSettings.copy()
    downsample_factor = objSettings.get('downsample_factor', 2)
    upsample_factor = objSettings.get('upsample_factor', 1)
    if objOutput is None:
        return 'initialized'
    if 'npyImage' in objScratch:
        pil_img = Image.fromarray(objScratch['npyImage'].copy()).convert("RGB")
        objScratch['image'] = pil_img
        # ä½¿ç”¨å¯è°ƒèŠ‚çš„ä¸‹é‡‡æ ·å€ç‡
        cond_height = pil_img.height // downsample_factor
        cond_width = pil_img.width // downsample_factor
        objScratch['cond_image'] = transforms.Resize((cond_height, cond_width))(pil_img)
        if upsample_factor > 1:
            cond_height = cond_height * upsample_factor
            cond_width = cond_width * upsample_factor   
            objScratch['cond_image'] = transforms.Resize((cond_height, cond_width))(objScratch['cond_image'])
        
def output_data(objScratch, objOutput):
    if objOutput is None:
        return 'initialized'
    if 'image' in objScratch:
        objOutput['image'] = transforms.ToTensor()(objScratch['image'])
    if 'cond_image' in objScratch:
        objOutput['cond_image'] = transforms.ToTensor()(objScratch['cond_image'])
    if 'strImagehash' in objScratch:
        objOutput['strImagehash'] = objScratch['strImagehash']

def collate_fn(batch):
    return {
        'image': torch.stack([x['image'] for x in batch]),
        'cond_image': torch.stack([x['cond_image'] for x in batch]),
        '__key__': [x['strImagehash'] for x in batch],
    }


def create_train_dataloader(config=None):
    """åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒæœ¬åœ°å›¾åƒå’ŒS3æ•°æ®ï¼‰"""
    # æ ¹æ®ä¸‹é‡‡æ ·å€ç‡è®¡ç®—multiple_of
    downsample_factor = config.experiment.get("downsample_factor", 2)
    upsample_factor = config.experiment.get("upsample_factor", 1)
    multiple_of = 16 * downsample_factor

    if config is None:
        # é»˜è®¤å‚æ•° - ä½¿ç”¨S3æ•°æ®
        batch_size = 5
        num_workers = 4
        num_threads = 8
        query_files = ['s3://sniklaus-clio-query/*/origin=si']
        
        return Dataloader(
            intBatchsize=batch_size,
            intWorkers=num_workers,
            intThreads=num_threads,
            strQueryfile=query_files,
            funcGroup=functools.partial(piat.meta_groupaspects, {'intSize': 1024, 'intMultiple': multiple_of, 'strResize': 'preserve-area'}),
            funcStages=[
                functools.partial(piat.filter_image, {'strCondition': 'fltPhoto > 0.9', 'strCondition': 'fltAesthscore > 4.0', 'strCondition': 'fltGenai < 0.05'}),
                functools.partial(piat.image_load, {'strSource': '1024-pil-antialias'}),
                functools.partial(piat.image_resize_antialias, {'intSize': 1024}),
                functools.partial(piat.image_crop_smart, {'intSize': 1024}),
                functools.partial(attach_images, {'downsample_factor': downsample_factor, 'upsample_factor': upsample_factor}),
                output_data,
            ],
            intSeed=random.randint(0, 1000000),
            collate_fn=collate_fn,
        )
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æœ¬åœ°è®­ç»ƒ
    if hasattr(config, 'local_train') and config.local_train.enabled:
        print(f"ä½¿ç”¨æœ¬åœ°å›¾åƒè®­ç»ƒï¼Œå›¾åƒç›®å½•: {config.local_train.image_dir}")
        print(f"ä¸‹é‡‡æ ·å€ç‡: {downsample_factor}, multiple_of: {multiple_of}")
        
        # ä¼˜å…ˆä½¿ç”¨local_trainä¸­çš„ä¸“ç”¨å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°dataloaderé…ç½®
        batch_size = getattr(config.local_train, 'batch_size', config.dataloader.train.batch_size)
        num_workers = getattr(config.local_train, 'num_workers', config.dataloader.train.num_workers)
        
        # è·å–max_sampleså‚æ•°ï¼Œç”¨äºoverfitting
        max_samples = getattr(config.local_train, 'max_samples', None)
        if max_samples is not None:
            print(f"ğŸ”’ Overfittingæ¨¡å¼ï¼šé™åˆ¶ä½¿ç”¨å‰ {max_samples} å¼ å›¾åƒ")
        
        print(f"æœ¬åœ°è®­ç»ƒå‚æ•° - batch_size: {batch_size}, num_workers: {num_workers}")
        
        return create_bucket_dataloader(
            image_folder_path=config.local_train.image_dir,
            base_size=1024,  # ä¸ S3 æ•°æ®ä¿æŒä¸€è‡´
            multiple_of=multiple_of,   # æ ¹æ®ä¸‹é‡‡æ ·å€ç‡åŠ¨æ€è°ƒæ•´
            batch_size=batch_size,
            num_workers=num_workers,
            drop_last=False,
            downsample_factor=downsample_factor,
            upsample_factor=upsample_factor,
            max_samples=max_samples
        )
    else:
        # ä½¿ç”¨S3æ•°æ®
        # ä»é…ç½®ä¸­è¯»å–å‚æ•°
        dataloader_config = config.dataloader.train
        batch_size = dataloader_config.batch_size
        num_workers = dataloader_config.num_workers
        num_threads = dataloader_config.num_threads
        query_files = dataloader_config.query_files
        
        return Dataloader(
            intBatchsize=batch_size,
            intWorkers=num_workers,
            intThreads=num_threads,
            strQueryfile=query_files,
            funcGroup=functools.partial(piat.meta_groupaspects, {'intSize': 1024, 'intMultiple': multiple_of, 'strResize': 'preserve-area'}),
            funcStages=[
                functools.partial(piat.filter_image, {'strCondition': 'fltPhoto > 0.9', 'strCondition': 'fltAesthscore > 4.0', 'strCondition': 'fltGenai < 0.05'}),
                functools.partial(piat.image_load, {'strSource': '1024-pil-antialias'}),
                functools.partial(piat.image_resize_antialias, {'intSize': 1024}),
                functools.partial(piat.image_crop_smart, {'intSize': 1024}),
                functools.partial(attach_images, {'downsample_factor': downsample_factor, 'upsample_factor': upsample_factor}),
                output_data,
            ],
            intSeed=random.randint(0, 1000000),
            collate_fn=collate_fn,
        )


def create_local_train_dataloader(image_dir, batch_size=5, num_workers=4, base_size=1024, downsample_factor=2, drop_last=True, shuffle=True, max_samples=None):
    """
    åˆ›å»ºæœ¬åœ°è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨ bucket dataloaderï¼‰
    
    Args:
        image_dir (str): å›¾åƒç›®å½•è·¯å¾„
        batch_size (int): æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤ä¸º5
        num_workers (int): å·¥ä½œè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸º4
        base_size (int): åŸºç¡€å°ºå¯¸ï¼Œé»˜è®¤ä¸º1024
        downsample_factor (int): ä¸‹é‡‡æ ·å€ç‡ï¼Œé»˜è®¤ä¸º2
        drop_last (bool): æ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡ï¼Œé»˜è®¤ä¸ºTrue
        shuffle (bool): æ˜¯å¦éšæœºåŒ–é¡ºåºï¼ŒTrueä¸ºè®­ç»ƒæ¨¡å¼ï¼ŒFalseä¸ºéªŒè¯æ¨¡å¼ï¼Œé»˜è®¤ä¸ºTrue
        max_samples (int): æœ€å¤§å›¾åƒæ•°é‡ï¼Œç”¨äºoverfittingæ—¶é™åˆ¶æ•°æ®é›†å¤§å°
        
    Returns:
        DataLoader: æœ¬åœ°å›¾åƒæ•°æ®åŠ è½½å™¨
    """
    # æ ¹æ®ä¸‹é‡‡æ ·å€ç‡è®¡ç®—multiple_of
    multiple_of = 16 * downsample_factor
    
    return create_bucket_dataloader(
        image_folder_path=image_dir,
        base_size=base_size,
        multiple_of=multiple_of,
        batch_size=batch_size,
        num_workers=num_workers,
        drop_last=drop_last,
        downsample_factor=downsample_factor,
        shuffle=shuffle,
        max_samples=max_samples
    )


def create_val_dataloader(config=None):
    """åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒæœ¬åœ°å›¾åƒå’ŒS3æ•°æ®ï¼‰"""
    # æ ¹æ®ä¸‹é‡‡æ ·å€ç‡è®¡ç®—multiple_of
    downsample_factor = config.experiment.get("downsample_factor", 2)
    upsample_factor = config.experiment.get("upsample_factor", 1)
    multiple_of = 16 * downsample_factor
    if config is None:
        # é»˜è®¤å‚æ•° - ä½¿ç”¨S3æ•°æ®
        batch_size = 4
        num_workers = 2
        num_threads = 4
        query_files = ['s3://sniklaus-clio-query/*/origin=sf&offensiveness=0&genairemoval=date&keywordblocklist=v1']
        
        return Dataloader(
            intBatchsize=batch_size,
            intWorkers=num_workers,
            intThreads=num_threads,
            strQueryfile=query_files,
            intSeed=0,
            funcGroup=functools.partial(piat.meta_groupaspects, {'intSize': 1024, 'intMultiple': multiple_of, 'strResize': 'preserve-area'}),
            funcStages=[
                functools.partial(piat.filter_image, {'strCondition': 'fltPhoto > 0.9', 'strCondition': 'fltAesthscore > 5.0'}),
                functools.partial(piat.image_load, {'strSource': '1024-pil-antialias'}),
                functools.partial(piat.image_resize_antialias, {'intSize': 1024}),
                functools.partial(piat.image_crop_smart, {'intSize': 1024}),
                functools.partial(attach_images, {'downsample_factor': downsample_factor, 'upsample_factor': upsample_factor}),
                output_data,
            ],
            collate_fn=collate_fn,
        )
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æœ¬åœ°è¯„ä¼°
    if hasattr(config, 'local_eval') and config.local_eval.enabled:
        print(f"ä½¿ç”¨æœ¬åœ°å›¾åƒè¯„ä¼°ï¼Œå›¾åƒç›®å½•: {config.local_eval.image_dir}")
        print(f"ä¸‹é‡‡æ ·å€ç‡: {downsample_factor}, multiple_of: {multiple_of}")
        
        # ä¼˜å…ˆä½¿ç”¨local_evalä¸­çš„ä¸“ç”¨å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°dataloaderé…ç½®
        batch_size = getattr(config.local_eval, 'batch_size', config.dataloader.val.batch_size)
        num_workers = getattr(config.local_eval, 'num_workers', config.dataloader.val.num_workers)
        
        # è·å–max_sampleså‚æ•°ï¼Œç”¨äºoverfitting
        max_samples = getattr(config.local_eval, 'max_samples', None)
        if max_samples is not None:
            print(f"ğŸ”’ Overfittingæ¨¡å¼ï¼šé™åˆ¶ä½¿ç”¨å‰ {max_samples} å¼ å›¾åƒ")
        
        print(f"æœ¬åœ°è¯„ä¼°å‚æ•° - batch_size: {batch_size}, num_workers: {num_workers}")
        
        return create_bucket_dataloader(
            image_folder_path=config.local_eval.image_dir,
            base_size=1024,  # ä¸ S3 æ•°æ®ä¿æŒä¸€è‡´
            multiple_of=multiple_of,   # æ ¹æ®ä¸‹é‡‡æ ·å€ç‡åŠ¨æ€è°ƒæ•´
            batch_size=batch_size,
            num_workers=num_workers,
            drop_last=False,  # éªŒè¯æ—¶ä¸ä¸¢å¼ƒæœ€åä¸€ä¸ªæ‰¹æ¬¡
            downsample_factor=downsample_factor,
            upsample_factor=upsample_factor,
            shuffle=False,  # éªŒè¯é›†ä¿æŒå›ºå®šé¡ºåº
            max_samples=max_samples
        )
    else:
        # ä½¿ç”¨S3æ•°æ®
        # ä»é…ç½®ä¸­è¯»å–å‚æ•°
        dataloader_config = config.dataloader.val
        batch_size = dataloader_config.batch_size
        num_workers = dataloader_config.num_workers
        num_threads = dataloader_config.num_threads
        query_files = dataloader_config.query_files
        
        return Dataloader(
            intBatchsize=batch_size,
            intWorkers=num_workers,
            intThreads=num_threads,
            strQueryfile=query_files,
            intSeed=0,
            funcGroup=functools.partial(piat.meta_groupaspects, {'intSize': 1024, 'intMultiple': multiple_of, 'strResize': 'preserve-area'}),
            funcStages=[
                functools.partial(piat.filter_image, {'strCondition': 'fltPhoto > 0.9', 'strCondition': 'fltAesthscore > 5.0'}),
                functools.partial(piat.image_load, {'strSource': '1024-pil-antialias'}),
                functools.partial(piat.image_resize_antialias, {'intSize': 1024}),
                functools.partial(piat.image_crop_smart, {'intSize': 1024}),
                functools.partial(attach_images, {'downsample_factor': downsample_factor, 'upsample_factor': upsample_factor}),
                output_data,
            ],
            collate_fn=collate_fn,
        )


# # ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæ¥çš„å˜é‡å
# trainDataloader = create_train_dataloader()
# valDataloader = create_val_dataloader()

if __name__ == "__main__":
    # æµ‹è¯•åŸæœ‰çš„éªŒè¯æ•°æ®åŠ è½½å™¨
    print("æµ‹è¯•éªŒè¯æ•°æ®åŠ è½½å™¨:")
    for i, batch in enumerate(valDataloader):
        print(batch.keys())
        print(batch['image'].shape)
        print(batch['cond_image'].shape)
        if i > 10:
            break
    
    # æµ‹è¯•æœ¬åœ°è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼ˆå¦‚æœæœ‰æµ‹è¯•å›¾åƒç›®å½•ï¼‰
    print("\næµ‹è¯•æœ¬åœ°è®­ç»ƒæ•°æ®åŠ è½½å™¨:")
    try:
        test_dir = "/tmp/test_images"  # å¯ä»¥ä¿®æ”¹ä¸ºå®é™…çš„æµ‹è¯•ç›®å½•è·¯å¾„
        if os.path.exists(test_dir):
            local_train_loader = create_local_train_dataloader(
                image_dir=test_dir,
                batch_size=2,
                num_workers=0,
                base_size=512,  # æµ‹è¯•æ—¶ä½¿ç”¨è¾ƒå°çš„å°ºå¯¸
                downsample_factor=4
            )
            
            for i, batch in enumerate(local_train_loader):
                print(f"æ‰¹æ¬¡ {i}:")
                print(f"  å›¾åƒå½¢çŠ¶: {batch['image'].shape}")  # ä¿æŒä¸ S3 æ•°æ®æ ¼å¼ä¸€è‡´
                print(f"  æ¡ä»¶å›¾åƒå½¢çŠ¶: {batch['cond_image'].shape}")  # ä¿æŒä¸ S3 æ•°æ®æ ¼å¼ä¸€è‡´
                print(f"  å›¾åƒkey: {batch['__key__']}")
                if i >= 2:
                    break
            print("æœ¬åœ°è®­ç»ƒæ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆ!")
        else:
            print(f"æµ‹è¯•ç›®å½• {test_dir} ä¸å­˜åœ¨ï¼Œè·³è¿‡æœ¬åœ°è®­ç»ƒæ•°æ®åŠ è½½å™¨æµ‹è¯•")
    except Exception as e:
        print(f"æœ¬åœ°è®­ç»ƒæ•°æ®åŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}")